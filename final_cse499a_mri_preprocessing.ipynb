{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1267593,
          "sourceType": "datasetVersion",
          "datasetId": 723383
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T00:09:20.028555Z",
          "iopub.execute_input": "2024-11-21T00:09:20.029258Z",
          "iopub.status.idle": "2024-11-21T00:09:39.450910Z",
          "shell.execute_reply.started": "2024-11-21T00:09:20.029200Z",
          "shell.execute_reply": "2024-11-21T00:09:39.449754Z"
        },
        "id": "jdUTwdxv8xmt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# File path for Kaggle dataset\n",
        "file_path = \"/kaggle/input/brats2020-training-data/BraTS2020_training_data/content/data/meta_data.csv\"\n",
        "\n",
        "# Step 1: Load the original dataset\n",
        "original_meta_data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Show missing values before cleaning\n",
        "print(\"Missing values before cleaning:\\n\", original_meta_data.isnull().sum())\n",
        "\n",
        "# Step 3: Clean the data (drop duplicates, handle missing values, ensure numeric columns)\n",
        "meta_data = original_meta_data.copy()  # Make a copy for cleaning\n",
        "meta_data.drop_duplicates(inplace=True)\n",
        "meta_data.dropna(inplace=True)\n",
        "meta_data['target'] = pd.to_numeric(meta_data['target'], errors='coerce')\n",
        "meta_data['volume'] = pd.to_numeric(meta_data['volume'], errors='coerce')\n",
        "meta_data['slice'] = pd.to_numeric(meta_data['slice'], errors='coerce')\n",
        "\n",
        "# Step 4: Missing values after cleaning\n",
        "print(\"\\nMissing values after cleaning:\\n\", meta_data.isnull().sum())\n",
        "\n",
        "# Step 5: Compare row count before and after cleaning\n",
        "print(f\"\\nOriginal dataset row count: {original_meta_data.shape[0]}\")\n",
        "print(f\"Cleaned dataset row count: {meta_data.shape[0]}\")\n",
        "\n",
        "# Step 6: Plot distributions of numerical columns for comparison (before vs after cleaning)\n",
        "\n",
        "# Distribution for 'target', 'volume', 'slice' in the original data\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
        "\n",
        "# 'target' distribution\n",
        "sns.countplot(x='target', data=original_meta_data, ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Original Target Distribution')\n",
        "sns.countplot(x='target', data=meta_data, ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Cleaned Target Distribution')\n",
        "\n",
        "# 'volume' distribution\n",
        "sns.histplot(original_meta_data['volume'], kde=True, ax=axes[1, 0], color='blue')\n",
        "axes[1, 0].set_title('Original Volume Distribution')\n",
        "sns.histplot(meta_data['volume'], kde=True, ax=axes[1, 1], color='blue')\n",
        "axes[1, 1].set_title('Cleaned Volume Distribution')\n",
        "\n",
        "# 'slice' distribution\n",
        "sns.histplot(original_meta_data['slice'], kde=True, ax=axes[2, 0], color='green')\n",
        "axes[2, 0].set_title('Original Slice Distribution')\n",
        "sns.histplot(meta_data['slice'], kde=True, ax=axes[2, 1], color='green')\n",
        "axes[2, 1].set_title('Cleaned Slice Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Save the cleaned data (optional)\n",
        "cleaned_file_path = \"/kaggle/working/meta_data_cleaned.csv\"\n",
        "meta_data.to_csv(cleaned_file_path, index=False)\n",
        "print(f\"\\nPreprocessed data saved to: {cleaned_file_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T00:09:39.453197Z",
          "iopub.execute_input": "2024-11-21T00:09:39.453619Z",
          "iopub.status.idle": "2024-11-21T00:09:42.440415Z",
          "shell.execute_reply.started": "2024-11-21T00:09:39.453577Z",
          "shell.execute_reply": "2024-11-21T00:09:42.438230Z"
        },
        "id": "3exHpVP58xmx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# File path for Kaggle dataset\n",
        "file_path = \"/kaggle/input/brats2020-training-data/BraTS2020_training_data/content/data/name_mapping.csv\"\n",
        "\n",
        "# Step 1: Load the original dataset\n",
        "original_name_mapping = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Show missing values before cleaning\n",
        "print(\"Missing values before cleaning:\\n\", original_name_mapping.isnull().sum())\n",
        "\n",
        "# Step 3: Remove duplicates\n",
        "name_mapping = original_name_mapping.copy()  # Make a copy for cleaning\n",
        "name_mapping.drop_duplicates(inplace=True)\n",
        "\n",
        "# Step 4: Handle missing values (Drop rows with missing values for simplicity)\n",
        "name_mapping.dropna(inplace=True)\n",
        "\n",
        "# Step 5: Ensure correct formatting for columns (e.g., `BraTS_2017_subject_ID` etc.)\n",
        "# If any columns are meant to be numeric, you can convert them as follows (assuming ID columns are strings):\n",
        "name_mapping['BraTS_2017_subject_ID'] = name_mapping['BraTS_2017_subject_ID'].astype(str)\n",
        "name_mapping['BraTS_2018_subject_ID'] = name_mapping['BraTS_2018_subject_ID'].astype(str)\n",
        "name_mapping['BraTS_2019_subject_ID'] = name_mapping['BraTS_2019_subject_ID'].astype(str)\n",
        "name_mapping['BraTS_2020_subject_ID'] = name_mapping['BraTS_2020_subject_ID'].astype(str)\n",
        "\n",
        "# Step 6: Validate file paths (optional) - Check if the paths in 'BraTS_2017_subject_ID' column exist\n",
        "invalid_paths = [path for path in name_mapping['BraTS_2017_subject_ID'] if not os.path.exists(path)]\n",
        "if invalid_paths:\n",
        "    print(f\"Invalid paths found: {len(invalid_paths)}\")\n",
        "else:\n",
        "    print(\"All paths are valid.\")\n",
        "\n",
        "# Step 7: Missing values after cleaning\n",
        "print(\"\\nMissing values after cleaning:\\n\", name_mapping.isnull().sum())\n",
        "\n",
        "# Step 8: Compare row count before and after cleaning\n",
        "print(f\"\\nOriginal dataset row count: {original_name_mapping.shape[0]}\")\n",
        "print(f\"Cleaned dataset row count: {name_mapping.shape[0]}\")\n",
        "\n",
        "# Step 9: Visualize distribution of 'Grade' (or other categorical features)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Grade', data=name_mapping)\n",
        "plt.title(\"Distribution of Grades in the Cleaned Data\")\n",
        "plt.xlabel('Grade')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 10: Save the cleaned data\n",
        "cleaned_file_path = \"/kaggle/working/name_mapping_cleaned.csv\"\n",
        "name_mapping.to_csv(cleaned_file_path, index=False)\n",
        "print(f\"\\nPreprocessed data saved to: {cleaned_file_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T00:11:40.854964Z",
          "iopub.execute_input": "2024-11-21T00:11:40.856096Z",
          "iopub.status.idle": "2024-11-21T00:11:41.163382Z",
          "shell.execute_reply.started": "2024-11-21T00:11:40.856052Z",
          "shell.execute_reply": "2024-11-21T00:11:41.161817Z"
        },
        "id": "dJ2H9mHk8xmy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# File path for Kaggle dataset\n",
        "file_path = \"/kaggle/input/brats2020-training-data/BraTS2020_training_data/content/data/survival_info.csv\"\n",
        "\n",
        "# Step 1: Load the original dataset\n",
        "original_survival_info = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Show missing values before cleaning\n",
        "print(\"Missing values before cleaning:\\n\", original_survival_info.isnull().sum())\n",
        "\n",
        "# Step 3: Remove duplicates\n",
        "survival_info = original_survival_info.copy()  # Make a copy for cleaning\n",
        "survival_info.drop_duplicates(inplace=True)\n",
        "\n",
        "# Step 4: Handle missing values (drop rows with missing values for simplicity)\n",
        "survival_info.dropna(inplace=True)\n",
        "\n",
        "# Step 5: Ensure correct formatting for columns (e.g., `Age` and `Survival_days`)\n",
        "survival_info['Age'] = pd.to_numeric(survival_info['Age'], errors='coerce')\n",
        "survival_info['Survival_days'] = pd.to_numeric(survival_info['Survival_days'], errors='coerce')\n",
        "\n",
        "# Step 6: Check if there are any rows with invalid data (NA values after coercion)\n",
        "survival_info.dropna(inplace=True)\n",
        "\n",
        "# Step 7: Missing values after cleaning\n",
        "print(\"\\nMissing values after cleaning:\\n\", survival_info.isnull().sum())\n",
        "\n",
        "# Step 8: Compare row count before and after cleaning\n",
        "print(f\"\\nOriginal dataset row count: {original_survival_info.shape[0]}\")\n",
        "print(f\"Cleaned dataset row count: {survival_info.shape[0]}\")\n",
        "\n",
        "# Step 9: Visualize the distribution of 'Age' and 'Survival_days'\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Age Distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(survival_info['Age'], kde=True, color='blue')\n",
        "plt.title('Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Survival Days Distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(survival_info['Survival_days'], kde=True, color='green')\n",
        "plt.title('Survival Days Distribution')\n",
        "plt.xlabel('Survival Days')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 10: Visualizing the Extent of Resection\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Extent_of_Resection', data=survival_info, palette=\"Set2\")\n",
        "plt.title(\"Extent of Resection Distribution\")\n",
        "plt.xlabel('Extent of Resection')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 11: Save the cleaned data\n",
        "cleaned_file_path = \"/kaggle/working/survival_info_cleaned.csv\"\n",
        "survival_info.to_csv(cleaned_file_path, index=False)\n",
        "print(f\"\\nPreprocessed data saved to: {cleaned_file_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T00:11:52.168585Z",
          "iopub.execute_input": "2024-11-21T00:11:52.169010Z",
          "iopub.status.idle": "2024-11-21T00:11:53.370667Z",
          "shell.execute_reply.started": "2024-11-21T00:11:52.168973Z",
          "shell.execute_reply": "2024-11-21T00:11:53.368688Z"
        },
        "id": "Mc3JlJAq8xmz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# File path for the BraTS Training Metadata dataset\n",
        "file_path = '/kaggle/input/brats2020-training-data/BraTS20 Training Metadata.csv'\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "metadata = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Display the first few rows of the dataset to understand its structure\n",
        "print(metadata.head())\n",
        "\n",
        "# Step 3: Check for missing values\n",
        "print(\"\\nMissing values before cleaning:\\n\", metadata.isnull().sum())\n",
        "\n",
        "# Step 4: Remove duplicates if any\n",
        "metadata.drop_duplicates(inplace=True)\n",
        "\n",
        "# Step 5: Handle missing values by dropping rows with NaN (optional: use imputation for specific columns)\n",
        "metadata.dropna(inplace=True)\n",
        "\n",
        "# Step 6: Convert data types where necessary\n",
        "metadata['label0_pxl_cnt'] = pd.to_numeric(metadata['label0_pxl_cnt'], errors='coerce')\n",
        "metadata['label1_pxl_cnt'] = pd.to_numeric(metadata['label1_pxl_cnt'], errors='coerce')\n",
        "metadata['label2_pxl_cnt'] = pd.to_numeric(metadata['label2_pxl_cnt'], errors='coerce')\n",
        "metadata['background_ratio'] = pd.to_numeric(metadata['background_ratio'], errors='coerce')\n",
        "\n",
        "# Step 7: Check if data is now clean\n",
        "print(\"\\nMissing values after cleaning:\\n\", metadata.isnull().sum())\n",
        "\n",
        "# Step 8: Visualize distributions of key columns (e.g., pixel counts, background ratio)\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "# Distribution of label pixel counts\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.histplot(metadata['label0_pxl_cnt'], kde=True, color='blue')\n",
        "plt.title('Label 0 Pixel Count Distribution')\n",
        "plt.xlabel('Label 0 Pixel Count')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.histplot(metadata['label1_pxl_cnt'], kde=True, color='green')\n",
        "plt.title('Label 1 Pixel Count Distribution')\n",
        "plt.xlabel('Label 1 Pixel Count')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.histplot(metadata['label2_pxl_cnt'], kde=True, color='red')\n",
        "plt.title('Label 2 Pixel Count Distribution')\n",
        "plt.xlabel('Label 2 Pixel Count')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 9: Visualize background ratio distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(metadata['background_ratio'], kde=True, color='purple')\n",
        "plt.title('Background Ratio Distribution')\n",
        "plt.xlabel('Background Ratio')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Step 10: Correlation heatmap for numerical features\n",
        "plt.figure(figsize=(10, 8))\n",
        "correlation_matrix = metadata[['label0_pxl_cnt', 'label1_pxl_cnt', 'label2_pxl_cnt', 'background_ratio']].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# Step 11: Pairplot for numerical features\n",
        "sns.pairplot(metadata[['label0_pxl_cnt', 'label1_pxl_cnt', 'label2_pxl_cnt', 'background_ratio']])\n",
        "plt.suptitle('Pairplot for Pixel Counts and Background Ratio', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Step 12: Countplot for categorical features ('target' and 'slice')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='target', data=metadata, palette='Set2')\n",
        "plt.title('Distribution of Target Class')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='slice', data=metadata, palette='Set2')\n",
        "plt.title('Distribution of Slices')\n",
        "plt.xlabel('Slice')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Step 13: Violin Plot for the label pixel counts\n",
        "plt.figure(figsize=(18, 6))\n",
        "sns.violinplot(x='target', y='label0_pxl_cnt', data=metadata, palette='Blues')\n",
        "plt.title('Violin Plot of Label 0 Pixel Count by Target Class')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Label 0 Pixel Count')\n",
        "plt.show()\n",
        "\n",
        "# Step 14: Scatter plot between label pixel counts\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='label0_pxl_cnt', y='label1_pxl_cnt', data=metadata, hue='target', palette='Set2')\n",
        "plt.title('Scatter Plot of Label 0 vs Label 1 Pixel Count')\n",
        "plt.xlabel('Label 0 Pixel Count')\n",
        "plt.ylabel('Label 1 Pixel Count')\n",
        "plt.show()\n",
        "\n",
        "# Step 15: Save the cleaned dataset\n",
        "cleaned_metadata_file_path = '/kaggle/working/BraTS20_Training_Metadata_cleaned.csv'\n",
        "metadata.to_csv(cleaned_metadata_file_path, index=False)\n",
        "print(f\"\\nCleaned dataset saved to: {cleaned_metadata_file_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T00:14:04.204686Z",
          "iopub.execute_input": "2024-11-21T00:14:04.205178Z",
          "iopub.status.idle": "2024-11-21T00:17:29.692424Z",
          "shell.execute_reply.started": "2024-11-21T00:14:04.205143Z",
          "shell.execute_reply": "2024-11-21T00:17:29.690921Z"
        },
        "id": "ABYMkLou8xm0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# File path to the HDF5 file on Kaggle (adjust the path as needed)\n",
        "h5_file_path = \"/kaggle/input/brats2020-training-data/BraTS2020_training_data/content/data/volume_100_slice_0.h5\"\n",
        "\n",
        "# Open the HDF5 file and explore its structure\n",
        "with h5py.File(h5_file_path, 'r') as f:\n",
        "    # List all keys (datasets or groups) in the file\n",
        "    print(\"Keys in the file:\", list(f.keys()))\n",
        "\n",
        "    # Check for 'image' dataset\n",
        "    image_dataset_name = 'image'\n",
        "    mask_dataset_name = 'mask'\n",
        "\n",
        "    if image_dataset_name in f.keys():\n",
        "        image_dataset = f[image_dataset_name]\n",
        "\n",
        "        # Inspect the shape and properties of the image dataset\n",
        "        print(f\"Dataset: {image_dataset_name}\")\n",
        "        print(f\"Shape: {image_dataset.shape}\")\n",
        "        print(f\"Number of dimensions: {len(image_dataset.shape)}\")\n",
        "\n",
        "        # Check if the image dataset is 2D or 3D\n",
        "        if len(image_dataset.shape) == 2:\n",
        "            print(\"This is a 2D image dataset.\")\n",
        "            # Example: Normalize the image data for 2D\n",
        "            image_data = image_dataset[:]\n",
        "            image_data_normalized = image_data / np.max(image_data)  # Normalize by max value\n",
        "            print(\"Image data normalized (2D).\")\n",
        "        elif len(image_dataset.shape) == 3:\n",
        "            print(\"This is a 3D image dataset.\")\n",
        "            # Example: Normalize the image data for 3D\n",
        "            image_data = image_dataset[:]\n",
        "            image_data_normalized = image_data / np.max(image_data)  # Normalize by max value\n",
        "            print(\"Image data normalized (3D).\")\n",
        "        else:\n",
        "            print(\"Unexpected image dataset structure.\")\n",
        "\n",
        "        # Optionally, save the normalized image data to a new file\n",
        "        save_path = \"/kaggle/working/normalized_volume_100_slice_0_image.h5\"\n",
        "        with h5py.File(save_path, 'w') as new_f:\n",
        "            new_f.create_dataset('normalized_image', data=image_data_normalized)\n",
        "            print(f\"Normalized image data saved to: {save_path}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Dataset '{image_dataset_name}' not found in the file.\")\n",
        "\n",
        "    # Check for 'mask' dataset\n",
        "    if mask_dataset_name in f.keys():\n",
        "        mask_dataset = f[mask_dataset_name]\n",
        "\n",
        "        # Inspect the shape and properties of the mask dataset\n",
        "        print(f\"Dataset: {mask_dataset_name}\")\n",
        "        print(f\"Shape: {mask_dataset.shape}\")\n",
        "        print(f\"Number of dimensions: {len(mask_dataset.shape)}\")\n",
        "\n",
        "        # Check if the mask dataset is 2D or 3D\n",
        "        if len(mask_dataset.shape) == 2:\n",
        "            print(\"This is a 2D mask dataset.\")\n",
        "            # Example: Normalize the mask data for 2D\n",
        "            mask_data = mask_dataset[:]\n",
        "            mask_data_normalized = mask_data / np.max(mask_data)  # Normalize by max value\n",
        "            print(\"Mask data normalized (2D).\")\n",
        "        elif len(mask_dataset.shape) == 3:\n",
        "            print(\"This is a 3D mask dataset.\")\n",
        "            # Example: Normalize the mask data for 3D\n",
        "            mask_data = mask_dataset[:]\n",
        "            mask_data_normalized = mask_data / np.max(mask_data)  # Normalize by max value\n",
        "            print(\"Mask data normalized (3D).\")\n",
        "        else:\n",
        "            print(\"Unexpected mask dataset structure.\")\n",
        "\n",
        "        # Optionally, save the normalized mask data to a new file\n",
        "        save_path = \"/kaggle/working/normalized_volume_100_slice_0_mask.h5\"\n",
        "        with h5py.File(save_path, 'w') as new_f:\n",
        "            new_f.create_dataset('normalized_mask', data=mask_data_normalized)\n",
        "            print(f\"Normalized mask data saved to: {save_path}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Dataset '{mask_dataset_name}' not found in the file.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T00:09:42.449646Z",
          "iopub.status.idle": "2024-11-21T00:09:42.450678Z",
          "shell.execute_reply.started": "2024-11-21T00:09:42.450374Z",
          "shell.execute_reply": "2024-11-21T00:09:42.450405Z"
        },
        "id": "bahGex6Y8xm0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List all .h5 files\n",
        "all_files = [file for file in os.listdir(data_folder) if file.endswith('.h5')]\n",
        "\n",
        "# Shuffle the files for random splitting\n",
        "random.shuffle(all_files)\n",
        "\n",
        "# Split into training (70%), validation (15%), and test (15%)\n",
        "train_size = int(0.7 * len(all_files))\n",
        "val_size = int(0.15 * len(all_files))\n",
        "test_size = len(all_files) - train_size - val_size\n",
        "\n",
        "# Define the splits\n",
        "train_files = all_files[:train_size]\n",
        "val_files = all_files[train_size:train_size + val_size]\n",
        "test_files = all_files[train_size + val_size:]\n",
        "\n",
        "# Plot the distribution in a pie chart\n",
        "sizes = [len(train_files), len(val_files), len(test_files)]\n",
        "labels = ['Training Set', 'Validation Set', 'Test Set']\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=['#66b3ff', '#99ff99', '#ffcc99'])\n",
        "plt.title(\"Dataset Splitting (Train-Val-Test)\")\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Print the counts for verification\n",
        "print(f\"Training Set: {len(train_files)} files\")\n",
        "print(f\"Validation Set: {len(val_files)} files\")\n",
        "print(f\"Test Set: {len(test_files)} files\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T00:19:32.164001Z",
          "iopub.execute_input": "2024-11-21T00:19:32.164519Z",
          "iopub.status.idle": "2024-11-21T00:19:32.403633Z",
          "shell.execute_reply.started": "2024-11-21T00:19:32.164481Z",
          "shell.execute_reply": "2024-11-21T00:19:32.402308Z"
        },
        "id": "bYGECPFU8xm1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# Define the path to the data folder and the output folder for processed files\n",
        "data_folder = '/kaggle/input/brats2020-training-data/BraTS2020_training_data/content/data/'\n",
        "processed_folder = '/kaggle/working/preprocessed_images/'\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "os.makedirs(processed_folder, exist_ok=True)\n",
        "\n",
        "# Loop through all H5 files in the data folder\n",
        "for file_name in os.listdir(data_folder):\n",
        "    if file_name.endswith('.h5'):  # Only process .h5 files\n",
        "        file_path = os.path.join(data_folder, file_name)\n",
        "\n",
        "        # Open the H5 file\n",
        "        with h5py.File(file_path, 'r') as h5_file:\n",
        "            if 'image' in h5_file:  # Ensure 'image' dataset exists\n",
        "                image_data = h5_file['image'][:]\n",
        "\n",
        "                # Normalize the image data (assuming it's a 3D volume)\n",
        "                image_data_normalized = image_data / np.max(image_data)\n",
        "\n",
        "                # Resize each 2D slice to a fixed size (e.g., 128x128)\n",
        "                resized_slices = [cv2.resize(slice, (128, 128)) for slice in image_data_normalized]\n",
        "                resized_slices = np.array(resized_slices)\n",
        "\n",
        "                # Ensure the shape is correct after resizing (add channel dimension if needed)\n",
        "                resized_slices = np.expand_dims(resized_slices, axis=-1)\n",
        "\n",
        "                # Save the processed image data into a new H5 file\n",
        "                save_image_path = os.path.join(processed_folder, file_name.replace('.h5', '_processed_image.h5'))\n",
        "                with h5py.File(save_image_path, 'w') as save_image_file:\n",
        "                    save_image_file.create_dataset('image', data=resized_slices)\n",
        "\n",
        "                print(f\"Processed and saved: {file_name}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-21T00:09:42.452227Z",
          "iopub.status.idle": "2024-11-21T00:09:42.452869Z",
          "shell.execute_reply.started": "2024-11-21T00:09:42.452550Z",
          "shell.execute_reply": "2024-11-21T00:09:42.452584Z"
        },
        "id": "pXEA8-qR8xm1"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}